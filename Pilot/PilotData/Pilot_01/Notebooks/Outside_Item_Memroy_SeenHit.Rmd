---
title: "Outside Item Memory"
output:
  html_document:
    fig_caption: yes
    highlight: default
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 5
    toc_float: 
      collapsed: true
      smooth_scroll: true
  header-includes:
  - \usepackage{xcolor}
  - \usepackage{framed}
  - \usepackage{fontspec}
  html_notebook:
    
    fig_caption: yes
    highlight: default
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 5
    toc_float: true
  pdf_document:
    fig_caption: yes
    highlight: default
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 5
---

\setmainfont{Calibri}
\colorlet{shadecolor}{gray!10}
\color{red}

<style>
body {
  font-size: 15px;
}

p.caption {
  font-weight: 500;
  font-style: italic;
}

code.r {
  font-family: Consolas, Monaco, monospace;
  font-size: 12px;
}
</style>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(kableExtra)
options(knitr.table.format = "html", cache=TRUE) # "latex" for PDF
```

> This notebook focuses on item memory outside the rooms for the intermediate version. 

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(data.table); library(ggplot2); library(nlme)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# load("~/Documents/OneDrive - Cardiff University/VRCuriosityMemory/PilotData/RawData/Pilot_01/.RData")
load("D:/OneDirve_CU/OneDrive - Cardiff University/Projects/VRCuriosityMemory/PilotData/RawData/Pilot_01/.RData")
```

# Overview of the data
The rooms were separated into *Familiar* and *Novel* groups. Participants explored the *Familiar* group at the beginning of the experiment. In the encoding task, they visisted both *Familiar* and *Novel* groups. Their item memory (i.e., in which room they saw the item) was tested in the memory test the next day of the encoding task. **Novelty benefits** on the item memory are examined by comparing the memory performance between the *Familiar* and *Novel* groups. 

Participants were also asked to rate how curious they felt about the room. The higher the rating is means they felt more curious about the room. Item memory as a function of curiosity rating was also examined for **curiosity benefits**.

In the encoding task, there were 6 objects in each room, and the participant saw the object after openning each of the six box. It is possbile that the item memory for the object to some extent depends on the order in which the object was seen. For example, it may be easier to remember where the first and last objects were seen as compared to those in the middle. 

# Visual inspection of the data
```{r, echo=FALSE, fig.align='center', fig.width=6.4, fig.height=5.4}
ggplot(Curiosity.Recall.Freq.Rsp, aes(x = Response, y = Frequency, group = CorrObjResp)) + theme_bw() +
  geom_point(size = 3, aes(color = CorrObjResp)) +
  geom_line(size = 0.75, aes(color = CorrObjResp)) +
  scale_x_discrete("Response", limits = c("Seen", "Familiar", "New")) +
  scale_color_brewer(palette = "Set1", labels = c("New items", "Old items")) +
  facet_wrap( ~ SubjectNo, ncol = 4) + 
  theme(strip.text = element_text(face = 'bold'),
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 0.5) )
```


```{r, echo=FALSE, fig.align='center', fig.width=7.2, fig.height=5.6}
ggplot(Curiosity.Recall.Freq.Grp, aes(x = Response, y = Frequency, group = Group)) + theme_bw() +
  geom_point(size = 3, aes(color = Group)) +
  geom_line(size = 0.75, aes(color = Group)) +
  scale_x_discrete("Response", limits = c("Seen", "Familiar", "New")) +
  scale_color_brewer(palette = "Set1", labels = c("Distractor", "In familiar room", "In novel room")) +
  facet_wrap( ~ SubjectNo, ncol = 4) + 
  theme(strip.text = element_text(face = 'bold'),
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 0.5) )
```

# Novelty Benefits 
We hypothesised that the memory for the items outside the rooms should be better for the *novel* rooms than for the *familiar* rooms. 

## Compare mean accuracy between *novel* and *familiar* rooms
```{r meanbars, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure1. Mean accuracy for item memory respectively for Familiar and Novel rooms.'}
ggplot(OutsideObjectsMemory[, .(MeanSeenHit = mean(SeenHit)), by = c("SubjectNo", "Group")], aes(x = Group, y = MeanSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  labs(x = "Room novelty", y = "Mean Hit Rate for Recollection")
```

Against our hypothesis, the mean accuracy of item memory is slightly higher in the *familiar* rooms than in the *novel* rooms. 

```{r, echo = FALSE}
t.test(MeanSeenHit ~ Group, data = OutsideObjectsMemory[, .(MeanSeenHit = mean(SeenHit)), by = c("SubjectNo", "Group")], paired = TRUE)
```
The difference didn't reach significance, but we only had 13 participants. 

## Item memory as a function of item order
### Visualise the data
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6, fig.cap="Figure 2. Mean hit rate of recollection at each order (i.e., 1st, 2nd, 3rd, etc), respectively for familiar and novel rooms."}
ggplot(Curiosity.Recall.Old, aes(x = ItemOrder, y = SeenHit)) +
  stat_summary(fun.y = mean, geom = "point", aes(group = Group, colour = Group), position = position_dodge(width = 0.2), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, aes(group = Group, colour = Group)) +
  labs(x = "Item order", y = "Hit rate for recollection") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```
For *novel* rooms, the first and last objects were remembered better and the memory for the middle ones (especially 3rd ones) were the worst. For the *familiar* rooms, the objects presented earlier were remembered better. `r text_spec("Note that we only have 13 participants here", background = "#D05A6E", color = "white", bold = T, font_size = 15)`. 

### Testing novelty effects by fitting mixed models
Build models with recollection hit rate as $y$ and examine effects of item order and novelty by including the predictors one by one and then comparing the models. 

* `baseline.item` is the model with only an intercept. 
* `order.model.linear.1` is the model with $\small ItemOrder$ included, showing the effects of item order. 
* `order.model.linear.2` is the model allowing the slope (i.e., relationship between item memory and item order) varying across participants. 
* `order.model.quad.1` and `order.model.quad.2` repectively included $\small ItemOrder^2$ and related random effects. 
* `novelty.model.1` is the model with $\small Group$ (i.e., *familiar* and *novel* rooms) included, examining the effects of novelty of the rooms on the intercept (i.e., mean item memory accuracy at the 1st order or overall if no difference in slope). 
* `novelty.model.2` is the model with interaction between $\small Group$ and $\small ItemOrder$ included, examining the effects of novelty on how item memory accuracy changes as a function of item order. 
* `novelty.model.3` included interaction between $\small Group$ and $\small ItemOrder^2$ for inestigating whether the pattern changes between *familiar* and *novel* groups. 

```{r, echo = FALSE}
baseline.item <- lme(SeenHit ~ 1, random = ~ 1|SubjectNo, data = Curiosity.Recall.Old, method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

order.model.linear.1 <- update(baseline.item, .~. + ItemOrder)
order.model.linear.2 <- update(order.model.linear.1, random = ~ 1 + ItemOrder|SubjectNo)

order.model.quad.1   <- update(order.model.linear.2, .~. + I(ItemOrder^2))
order.model.quad.2   <- update(order.model.quad.1, random = ~ 1 + ItemOrder + I(ItemOrder^2)|SubjectNo)

novelty.model.1 <- update(order.model.quad.2, .~. + Group)
novelty.model.2 <- update(novelty.model.1, .~. + Group:ItemOrder)
novelty.model.3 <- update(novelty.model.2, .~. + Group:I(ItemOrder^2))

anova(baseline.item, order.model.linear.1, order.model.linear.2, order.model.quad.1, order.model.quad.2, novelty.model.1, novelty.model.2, novelty.model.3)
```

#### Parameter estimation for the complete model
```{r, echo=FALSE}
summary(novelty.model.3, correlation = FALSE)
```

In the complete model that contains all the predictors, none of the predictors has a significant estimated parameter value. It may be mainly we had only 13 participants now and less power due to small prediction-participant ratio. 

# Curiosity Benefits
We hypothesed that the higher the curiosity rating was, the better the item memory should be.

## Mean accuracy of item memory as a function of curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure 3. Mean accuracy for item memory as a function of curiosity rating.'}
ggplot(OutsideObjectsMemory, aes(x = CuriosityRating, y = SeenHit)) +
  stat_summary(fun.data = mean_cl_boot, na.rm = T, geom = "errorbar", width = 0.2) +
  stat_summary(fun.y = mean, na.rm = T, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(1:7)) +
  labs(x = "Curiosity rating", y = "Mean accuracy for item memory") 
```
However, the trend looks opposite to our prediction. 

### Testing the relationship between mean accuracy of item memory and curiosity rating
Build models with item memory accuracy as $y$ and examine effects of curiosity rating by including it into the model and then comparing the model fits. 

* `baseline.rating` is the model with only an intercept, showing no change in item memory as a function of curiosity rating. 
* `rating.intercept` is the model with $\small CuriosityRating$ included, showing the effects of curiosity rating. 
* `rating.slope` is the model allowing the slope (i.e., relationship between item memory and curiosity rating) varying across participants. 

```{r, echo = FALSE}
baseline.rating <- lme(SeenHit ~ 1, random = ~ 1|SubjectNo, data = OutsideObjectsMemory, method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + CuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating|SubjectNo)

anova(baseline.rating, rating.intercept, rating.slope)
```
Adding $\small CuriosityRating$ into the model improved the model fit marginally significantly, suggesting it may play an important role.

#### Parameter estimates in the complete model
```{r}
summary(rating.slope)
```
The estimated parameter for the $\small CuriosityRating$ is negative, and significant, suggesting there is a negative correlation between item memory and curiosity rating, against our prediction. 

#### Plot the model
```{r, echo = FALSE, fig.align='center', fig.width=6.4, fig.height=4.8}
fixef.curiosity.item <- fixef(rating.intercept)
fit.curiosity.item <- fixef.curiosity.item[[1]] + c(1:7) * fixef.curiosity.item[[2]]
plot(c(1:7), fit.curiosity.item, ylim = c(0, 1), type = "b", ylab = "predicted item memory accuracy", xlab = "curiosity rating")
title("Relationship between Curiosity Rating and Item Memory")
```

There seems to be a negative correlation between curiosity rating and item memory.

## Does the relationship with curiosity rating depend on item order?
```{r, echo=FALSE, fig.align='center', fig.width=10.8, fig.height=3, fig.cap='Figure 4. Mean accuracy across participants for item memory as a function of curiosity rating for each order at which the item was seen (I.e., 1st, 2nd, 3rd, etc.).'}
ggplot(Curiosity.Recall.Old, aes(x = CuriosityRating, y = SeenHit)) +
  stat_summary(fun.y = mean, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  labs(x = "Curiosity rating", y = "Accuracy of item Memory") +
  scale_x_continuous(breaks = c(1:7)) +
  facet_wrap( ~ ItemOrder, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```
Basically for all the item orders, there seems to be a negative correlation between curiosity rating and accuracy of item memory.

### Testing the effect of item order
Build models with context memory accuracy as $y$ and examine effects of curiosity rating and item order by comparing the models. 

* `item.model.linear.1` is the model with $\small ItemOrder$ included (in addition to $\small CuriosityRating$), showing the effects of item order. 
* `item.model.linear.2` is the model allowing the slope (i.e., relationship between item memory and item order) varying across participants. 
* `item.model.linear.3` is the model with the interaction between $\small ItemOrders$ and $\small CuriosityRating$ included.
* `item.model.quad.1` and `item.model.quad.2` repectively included $\small ItemOrder^2$ and related random effects. 
* `item.model.quad.3` included interaction between $\small ItemOrder^2$ and $\small CuriosityRating$ for inestigating whether the pattern changes as a function of *curiosity rating*. 

```{r}
rating.intercept <- update(baseline.item, .~. + CuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating | SubjectNo)

item.model.linear.1     <- update(rating.slope, .~. + ItemOrder)
item.model.linear.2     <- update(item.model.linear.1, random = list(~1+CuriosityRating|SubjectNo, ~1+ItemOrder|SubjectNo))
item.model.linear.3     <- update(item.model.linear.2, .~. + ItemOrder : CuriosityRating)

item.model.quad.1       <- update(item.model.linear.3, .~. + I(ItemOrder^2))
# item.model.quad.2       <- update(item.model.quad.1,   random = list(~1+CuriosityRating|SubjectNo, ~1+ItemOrder+I(ItemOrder^2)|SubjectNo)) - excluding this model because singularity issue (perhaps the model was getting too complex to be calculated)
item.model.quad.3       <- update(item.model.quad.1,   .~. + I(ItemOrder^2) : CuriosityRating)

anova(rating.intercept, rating.slope, item.model.linear.1, item.model.linear.2, item.model.linear.3, item.model.quad.1, item.model.quad.3)
```

#### Parameter estimation for the complete model
```{r}
summary(item.model.quad.3)
```

Maybe because of being lack of power (too many parameters and too few participants), no parameter has a significant estimated value.

### Collapse the order and curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6, fig.cap='Figure 7. Mean hit rate as a function of item order (Early vs. Late) and curiosity (Low vs. High).'}
ggplot(Curiosity.Recall.Old.Collapsed, aes(x = CuriosityRatingType, y = SeenHit)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Hit rate for recollection") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

#### Stats on it
```{r, echo = FALSE}
repeatedModel <- lme(SeenHit ~ CuriosityRatingType * ItemOrderType, random = ~ CuriosityRatingType + ItemOrderType | SubjectNo, data = Curiosity.Recall.Old.Collapsed, method = "ML", control = list(msMaxIter = 500, opt = "optim"))

summary(repeatedModel)
```

# Curiosity and novelty
## Testing the effects of curiosity rating and novelty together
Build models with recollection hit rate as $y$ and examine effects of curiosity rating and novelty together by including them into the model separately and then comparing the model fits. 

* `baseline.rating` is the model with only an intercept, showing no change in item memory as a function of curiosity rating. 
* `rating.intercept` is the model with $\small CuriosityRating$ included, showing the effects of curiosity rating. 
* `rating.slope` is the model allowing the slope (i.e., relationship between item memory and curiosity rating) varying across participants. 
* `novelty.model` is the model with $\small Group$ (i.e., *familiar* and *novel* rooms) included, examining the effects of novelty of the rooms on the intercept (i.e., mean item memory accuracy at the 1st order or overall if no difference in slope). 
* `interaction.model` is the model with interaction between $\small Group$ and $\small CuriosityRating$ included, examining the effects of novelty on how item memory accuracy changes as a function of item order. 

``` {r, echo = FALSE}
baseline.rating <- lme(SeenHit ~ 1, random = ~ 1|SubjectNo, data = OutsideObjectsMemory, method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + CuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating|SubjectNo)

novelty.model.1 <- update(rating.slope, .~. + Group)
novelty.model.2 <- update(novelty.model.1, random = list(~ 1 + CuriosityRating|SubjectNo, ~ 1 + Group|SubjectNo))
interaction.model <- update(novelty.model.2, .~. + Group:CuriosityRating)

anova(baseline.rating, rating.intercept, rating.slope, novelty.model.1, novelty.model.2, interaction.model)
```

### Check the complete model
```{r, echo=FALSE}
summary(novelty.model.2)
```


## What about the other way around
```{r, echo=FALSE}
novelty.model.1 <- update(baseline.rating, .~. + Group)
novelty.model.2 <- update(novelty.model.1, random = ~ 1 + Group | SubjectNo)

rating.model.1 <- update(novelty.model.2, .~. + CuriosityRating)
rating.model.2 <- update(rating.model.1, random = list( ~ 1 + CuriosityRating|SubjectNo, ~ 1 + Group | SubjectNo))

anova(baseline.rating, novelty.model.1, novelty.model.2, rating.model.1, rating.model.2)
```

### Check the model
```{r, echo = FALSE}
summary(rating.model.1)
```


