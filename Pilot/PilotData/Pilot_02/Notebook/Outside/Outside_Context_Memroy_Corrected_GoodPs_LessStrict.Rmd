---
title: "Outside Context Memory"
output:
  html_document:
    fig_caption: yes
    highlight: default
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 5
    toc_float: 
      collapsed: true
      smooth_scroll: true
  header-includes:
  - \usepackage{xcolor}
  - \usepackage{framed}
  - \usepackage{fontspec}
  html_notebook:
    
    fig_caption: yes
    highlight: default
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 5
    toc_float: true
  pdf_document:
    fig_caption: yes
    highlight: default
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 5
---

\setmainfont{Calibri}
\colorlet{shadecolor}{gray!10}
\color{red}

<style>
body {
  font-size: 15px;
}

p.caption {
  font-weight: 500;
  font-style: italic;
}

code.r {
  font-family: Consolas, Monaco, monospace;
  font-size: 12px;
}
</style>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(kableExtra)
options(knitr.table.format = "html", cache=TRUE) # "latex" for PDF
```

> This notebook focuses on context memory outside the rooms. 
> The hit rate data was corrected by subtracting false alarm rate from the original hit rate *for each room*. In those cases when the corrected hit rate was negative (the hit rate was lower than the false alarm rate), the corrected hit rate was forced to be 0. 
> Note that the corrected hit rate is meaningful only when it was calculated as a mean.


```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(data.table); library(ggplot2); library(nlme); library(Hmisc); #library(ez)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# load("~/Documents/OneDrive - Cardiff University/Projects/VRCuriosityMemory/PilotData/RawData/Pilot_02/.RData")
load("D:/OneDirve_CU/OneDrive - Cardiff University/Projects/VRCuriosityMemory/PilotData/RawData/Pilot_02/.RData")
# load("D:/Danlu.C - CU OneDrive/OneDrive - Cardiff University/Projects/VRCuriosityMemory/PilotData/RawData/Pilot_02/.RData")
```

# Overview of the data
The rooms were separated into *Familiar* and *Novel* groups. Participants explored the *Familiar* group at the beginning of the experiment. In the encoding task, they visisted both *Familiar* and *Novel* groups. Their context memory (i.e., in which room they saw the item) was tested in the memory test the next day of the encoding task. **Novelty benefits** on the context memory are examined by comparing the memory performance between the *Familiar* and *Novel* groups. 

Participants were also asked to rate how curious they felt about the room. The higher the rating is means they felt more curious about the room. Context memory as a function of curiosity rating was also examined for **curiosity benefits**.

Questionnaires on personal traits regarding curiosity were also administered to participants before the encoding task (i.e., PC and EC). Scores on these questionnaires might be able to predict the context memory or its relationship with curiosity rating. 

In the encoding task, there were 6 objects outside each room. It is possbile that the context memory for the object to some extent depends on the order in which the object was seen. For example, it may be easier to remember where the first and last objects were seen as compared to those in the middle. Particularly, if the participant became more curious when getting closer to the room, then one would expect that the performance increases as a function of item order. 

```{r Find out bad participants, echo=FALSE}
# bad.ps.less.strict <- NULL
# 
# for (this.p in participant.folders) {
#   this.data <- Curiosity.Recall.Freq.Rsp[SubjectNo == this.p]
#   if (this.data[CorrObjResp == "New" & Response == "Seen"]$Frequency > this.data[CorrObjResp == "Seen" & Response == "Seen"]$Frequency) {
#     bad.ps.less.strict <- c(bad.ps.less.strict, this.p)
#   }
# }

OutsideObjectsMemory$Type2 <- "Good"
OutsideObjectsMemory[SubjectNo %in% bad.ps.less.strict]$Type2 <- "Bad"

# Curiosity.Recall.Old.Corr.Freq$Type2 <- "Good"
# Curiosity.Recall.Old.Corr.Freq[SubjectNo %in% bad.ps.less.strict]$Type2 <- "Bad"
```

# Individual performance
## An overview of individual context memory performance as a function of room.
```{r, echo=FALSE, fig.align='center', fig.width=7.2, fig.height=8.1, fig.cap='Figure 1. Mean accuracy for context memory respectively for each room and each individual participant.'}
ggplot(OutsideObjectsMemory, aes(x = Room, y = ContextAccuracy)) +
  geom_rect(data = subset(OutsideObjectsMemory,Type2 == 'Bad'),aes(fill = Type2),xmin = -Inf,xmax = Inf,ymin = -Inf,ymax = Inf,alpha = 0.1) +
  geom_line( aes(group = Group, colour = Group)) +
  geom_point( aes (colour = Group), size = 3) +
  facet_wrap( ~ SubjectNo, ncol = 4) +
  labs(x = "Room ", y = "Accuracy for context memory") +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(axis.text.x = element_text(angle = 45, size = 10, hjust = 1))
```

<!-- ## For correctly labelled distractor items -->
```{r, eval = FALSE, echo=FALSE, fig.align='center', fig.width=6.4, fig.height=4.2, fig.cap='Figure 2. Frequency of correctly labelled distractor objects for each individual participant.'}
ggplot(Curiosity.Recall.New.Freq, aes(x = Response, y = Frequency)) +
  geom_point( size = 1.5) +
  facet_wrap( ~ SubjectNo, ncol = 4) +
  labs(x = "Response ", y = "Frequency (%)") 
```

<!-- ## For correctly recalled items -->
```{r, eval = FALSE, echo=FALSE, fig.align='center', fig.width=7.2, fig.height=8.1, fig.cap='Figure 3. Frequency of correctly labelled distractor objects for each individual participant.'}
ggplot(Curiosity.Recall.Old.Corr.Freq, aes(x = Response, y = Frequency)) +
  geom_rect(data = subset(Curiosity.Recall.Old.Corr.Freq,Type2 == 'Bad'),aes(fill = Type2),xmin = -Inf,xmax = Inf,ymin = -Inf,ymax = Inf,alpha = 0.1) +
  geom_line( aes(group = Group, colour = Group)) +
  geom_point( size = 3, aes(group = Group, colour = Group)) +
  facet_wrap( ~ SubjectNo, ncol = 4) +
  labs(x = "Response ", y = "Frequency (%)") +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(axis.text.x = element_text(angle = 45, size = 10, hjust = 1))
```

# Novelty Benefits 
We predicted that the context memory should be better for the *novel* rooms than for the *familiar* rooms.

## Compare mean accuracy between *novel* and *familiar* rooms
```{r meanbars, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure 2. Mean accuracy for context memory respectively for Familiar and Novel rooms.'}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict, .(MeanCorrContextAccuracy = mean(CorrContextAccuracy)), by = c("SubjectNo", "Group")], aes(x = Group, y = MeanCorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  labs(x = "Room novelty", y = "Mean accuracy for context memory")
```

```{r, echo = FALSE}
t.test(MeanCorrContextAccuracy ~ Group, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict, .(MeanCorrContextAccuracy = mean(CorrContextAccuracy)), by = c("SubjectNo", "Group")], paired = TRUE)
```
There was no difference in average recollection performance for source memory between the *Familiar* and *Novel* rooms.

## Context memory as a function of item order
### Visualise the data
#### Nonlinear
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6, fig.cap="Figure 3. Mean accuracy of context memory for items at each order (i.e., 1st, 2nd, 3rd, etc), respectively for familiar and novel rooms."}
ggplot(Curiosity.Recall.Old.Novelty[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "point", aes(group = Group, colour = Group), position = position_dodge(width = 0.2), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, aes(group = Group, colour = Group)) +
  labs(x = "Item order", y = "Corrected hit rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

#### Linear
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6, fig.cap="Figure 4. Mean accuracy of context memory for items at each order (i.e., 1st, 2nd, 3rd, etc), respectively for familiar and novel rooms."}
ggplot(Curiosity.Recall.Old.Novelty[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "point", aes(group = Group, colour = Group), position = position_dodge(width = 0.2), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = Group, colour = Group)) +
  labs(x = "Item order", y = "Corrected hit rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

### Examing the relationship using mixed models.
Build models with corrected hit rate as $y$ and then examine effects of item order and novelty by comparing the models. 

* `baseline.context` is the model with only an intercept. 
* `order.model.linear.1` is the model with $\small ItemOrder$ included, showing the effects of item order. 
* `order.model.linear.2` is the model allowing the slope (i.e., relationship between context memory and item order) varying across participants. 
* `order.model.quad.1` and `order.model.quad.2` repectively included $\small ItemOrder^2$ and related random effects. 
* `novelty.model.1` is the model with $\small Group$ (i.e., *familiar* and *novel* rooms) included, examining the effects of novelty of the rooms on the intercept (i.e., mean context memory accuracy at the 1st order or overall if no difference in slope). 
* `novelty.model.2` is the model with interaction between $\small Group$ and $\small ItemOrder$ included, examining the effects of novelty on how context memory accuracy changes as a function of item order. 
* `novelty.model.3` included interaction between $\small Group$ and $\small ItemOrder^2$ for inestigating whether the pattern changes between *familiar* and *novel* groups. 

**Note: here I only examined the linear relationship.**

```{r, echo = FALSE}
# Centre the item order and make *Novel* as the baseline.
Curiosity.Recall.Old.Novelty$ItemOrderCt <- Curiosity.Recall.Old.Novelty$ItemOrder - 1

Curiosity.Recall.Old.Novelty$Group <- factor(Curiosity.Recall.Old.Novelty$Group)
contrasts(Curiosity.Recall.Old.Novelty$Group) <- c(1, 0)
```

```{r, echo = FALSE}
baseline.context <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo/Group, data = Curiosity.Recall.Old.Novelty[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

order.model.linear.1 <- update(baseline.context, .~. + ItemOrderCt)
# order.model.linear.2 <- update(order.model.linear.1, random = ~ 1 + ItemOrderCt|SubjectNo/Group)

# order.model.quad.1   <- update(order.model.linear.1, .~. + I(ItemOrder^2))
# order.model.quad.2   <- update(order.model.quad.1, random = ~ 1 + ItemOrder + I(ItemOrder^2)|SubjectNo)

novelty.model.1 <- update(order.model.linear.1, .~. + Group)
novelty.model.2 <- update(novelty.model.1, .~. + Group:ItemOrderCt)
# novelty.model.3 <- update(novelty.model.2, .~. + Group:I(ItemOrder^2))

anova(baseline.context, order.model.linear.1, novelty.model.1, novelty.model.2)
```

There was a significant interaction between *Novelty* and *Item order*, ${\chi}^2$(1)=13.91, *p* < 0.01.

#### Parameter estimation for the optimal model
```{r, echo=FALSE}
summary(novelty.model.2, correlation = FALSE)$tTable
```

From the parameters of the complete model:
* for *Novel* rooms, the performance decreased significantly against the item order, *b* = -0.025, *t*(398) = -3.17, *p* < 0.01. 
* For the first item that was saw on the pathway, the recollection was significantly worse for the *Familiar* rooms than for the *Novel* rooms, *b* = -0.10, *t*(39) = -2.97, *p* < 0.01.
* The change rate of recollection against item order was significantly larger (more positive)
 for the *Familiar* rooms than the *Novel* rooms, *b* = 0.042, *t*(398) = 3.74, *p* < 0.001.


### Collapse the order
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6, fig.cap='Figure 4. Mean hit rate as a function of item order (Early vs. Late) and curiosity (Low vs. High).'}
ggplot(Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict], aes(x = Group, y = CorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Hit rate for recollection") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

#### Stats on it
##### Using `lme`
```{r, echo = FALSE}
# Make *Novel* as the baseline.
Curiosity.Recall.Old.Order.Novelty$Group <- factor(Curiosity.Recall.Old.Order.Novelty$Group)
contrasts(Curiosity.Recall.Old.Order.Novelty$Group) <- c(1, 0)
```

```{r, echo = FALSE}
baseline         <- lme(CorrContextAccuracy ~ 1, random = ~ 1| SubjectNo/Group/ItemOrderType, data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE))

noveltyModel     <- update(baseline,     .~. + Group)
orderModel       <- update(noveltyModel, .~. + ItemOrderType)
interactionModel <- update(orderModel,   .~. + Group : ItemOrderType)

anova(baseline, noveltyModel, orderModel, interactionModel)
```
In line with the analysis above, there was a significant interaction between *Novelty* and *ItemOrder* (Early vs. Late), ${\chi}^2$(1)=13.20, *p* < 0.001.

```{r}
summary(interactionModel)$tTable
```
The parameters show:
* For the *Early* items the recollection was better for the *Novel* rooms than for the *Familiar* rooms, *b* = 0.063, *t*(39) = -2.26, *p* = 0.29.
* For the *Novel* rooms the recollection was better for the *Early* items than for the *Late* items, *b* = 0.077, *t*(78) = 3.05, *p* < 0.01.

##### Simple effects in "Early"
```{r}
t.test(CorrContextAccuracy ~ Group, data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict & ItemOrderType == "Early"], paired = T)
```
Looks a bit different from the parameter tests in the model above, but not much. It may be due to the way the data was included.

##### Simple effects in "Late"
```{r}
t.test(CorrContextAccuracy ~ Group, data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict & ItemOrderType == "Late"], paired = T)
```

# Curiosity Benefits
Here I checked how corrected hit rate changes as a function of curiosity rating. We hypothesised that the more curiosity the participant felt about the room (higher ratings) the better the context memory. 

## Mean accuracy of context memory as a function of curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure 5. Mean accuracy for context memory as a function of curiosity rating.'}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRating, y = CorrContextAccuracy)) +
  stat_summary(fun.data = mean_cl_boot, na.rm = T, geom = "errorbar", width = 0.2) +
  stat_summary(fun.y = mean, na.rm = T, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(1:7)) +
  labs(x = "Curiosity rating", y = "Corrected hit rate") 
```
It appears to be an overall negative correction between curiosity rating and source memory performance.

### Testing the relationship between mean accuracy of context memory and curiosity rating
Build models with context memory accuracy as $y$ and cexamine effects of curiosity rating by including it into the model and then comparing the model fits. 

* `baseline.rating` is the model with only an intercept, showing no change in context memory as a function of curiosity rating. 
* `rating.intercept` is the model with $\small CuriosityRating$ included, showing the effects of curiosity rating. 
* `rating.slope` is the model allowing the slope (i.e., relationship between context memory and curiosity rating) varying across participants. 

```{r, echo = FALSE}
baseline.rating <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + CuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating|SubjectNo)

anova(baseline.rating, rating.intercept, rating.slope)
```

Not significant.

### **Using normalised curiosity ratings.**
```{r, eval = FALSE, include = FALSE, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure 4. Mean accuracy for context memory as a function of curiosity rating.'}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], aes(x = NCuriosityRating, y = CorrContextAccuracy)) +
  stat_summary(fun.data = mean_cl_boot, na.rm = T, geom = "errorbar", width = 0.2) +
  stat_summary(fun.y = mean, na.rm = T, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  # scale_x_continuous(breaks = c(1:7)) +
  labs(x = "Normalised curiosity rating", y = "Corrected hit rate") 
```

```{r, echo = FALSE}
baseline.rating <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + NCuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + NCuriosityRating|SubjectNo)

anova(baseline.rating, rating.intercept, rating.slope)
```

Again, not significant.

## Does the relationship with curiosity rating depend on item order?
```{r, echo=FALSE, fig.align='center', fig.width=10.8, fig.height=3, fig.cap='Figure 6. Mean accuracy across participants for context memory as a function of curiosity rating for each order at which the item was seen (I.e., 1st, 2nd, 3rd, etc.).'}
ggplot(Curiosity.Recall.Old[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRating, y = CorrContextAccuracy)) + 
  stat_summary(fun.y = mean, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  labs(x = "Curiosity rating", y = "Accuracy of Context Memory") +
  scale_x_continuous(breaks = c(1:7)) +
  facet_wrap( ~ ItemOrder, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```
It seems that the relationship with the curiosity ratings changes as a function of item order - it becomes more negative with later item.

### Testing the effect of item order
Build models with corrected hit rate as $y$ and examine effects of curioity rating and item order by comparing the models. 

* `item.model.linear.1` is the model with $\small ItemOrder$ included (in addition to $\small CuriosityRating$), showing the effects of item order. 
* `item.model.linear.2` is the model allowing the slope (i.e., relationship between context memory and item order) varying across participants. 
* `item.model.linear.3` is the model with the interaction between $\small ItemOrders$ and $\small CuriosityRating$ included.
* `item.model.quad.1` and `item.model.quad.2` repectively included $\small ItemOrder^2$ and related random effects. 
* `item.model.quad.3` included interaction between $\small ItemOrder^2$ and $\small CuriosityRating$ for inestigating whether the pattern changes as a function of *curiosity rating*.

```{r, echo = FALSE}
Curiosity.Recall.Old$ItemOrderCt <- Curiosity.Recall.Old$ItemOrder - 1
Curiosity.Recall.Old$CuriosityRatingCt <- Curiosity.Recall.Old$CuriosityRating - 1
```

```{r}
baseline.context <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo, data = Curiosity.Recall.Old[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.context, .~. + CuriosityRatingCt)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRatingCt | SubjectNo)

item.model.linear.1     <- update(rating.slope, .~. + ItemOrderCt)
# item.model.linear.2     <- update(item.model.linear.1, random = list(~1+CuriosityRating|SubjectNo, ~1+ItemOrder|SubjectNo)) # Excluded because very little contribution to model fit improvement.
item.model.linear.3     <- update(item.model.linear.1, .~. + ItemOrderCt : CuriosityRatingCt)

# item.model.quad.1       <- update(item.model.linear.3, .~. + I(ItemOrder^2))
# item.model.quad.2       <- update(item.model.quad.1,   random = list(~1+CuriosityRating|SubjectNo, ~1+ItemOrder+I(ItemOrder^2)|SubjectNo)) # excluding this model because singularity issue (perhaps the model was getting too complex to be calculated)
# item.model.quad.3       <- update(item.model.quad.1,   .~. + I(ItemOrder^2) : CuriosityRating)

anova(baseline.context, rating.intercept, rating.slope, item.model.linear.1, item.model.linear.3)
```
Interaction was significant, ${\chi}^2$(1) = 9.73, *p* < 0.01.

#### Parameter estimation for the optimal model
```{r}
summary(item.model.linear.3)$tTable
```
For the first item on the pathway, there was a marginally significant correlation between source memory and curiosity ratings, *b* = 0.018, *t* = 1.77, *p* = 0.077.

The correlation got more negative as the distance between the item and room decreased, *b* = -0.01, *t* = -3.12, *p* < 0.01.

### Collapse the order and curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6, fig.cap='Figure 7. Mean hit rate as a function of item order (Early vs. Late) and curiosity (Low vs. High).'}
ggplot(Curiosity.Recall.Old.Order.CuriosityType[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRatingType, y = CorrContextAccuracy)) + theme_classic() +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Corrected Context Accuracy") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

#### Stats on it
```{r, echo = FALSE}
baseline      <- lme(CorrContextAccuracy ~ 1, random = ~1| SubjectNo/CuriosityRatingType/ItemOrderType, data = Curiosity.Recall.Old.Order.CuriosityType [!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE))

curiosityModel   <- update(baseline,         .~. + CuriosityRatingType)
orderModel       <- update(curiosityModel,   .~. + ItemOrderType)
interactionModel <- update(orderModel,       .~. + CuriosityRatingType : ItemOrderType)

anova(baseline, curiosityModel, orderModel, interactionModel)
```

A significant interaction, ${\chi}^2$(1) = 9.21, *p* < 0.01.

```{r}
summary(interactionModel)$tTable
```
For the early items, the recollection was higer for high curiosity, but not significant, *P* = 0.14.

##### Simple effects in "Early"
```{r}
# remove those with single curiosity ratings
temp.table <- as.data.frame(with(Curiosity.Recall.Old.Order.CuriosityType, table( SubjectNo, CuriosityRatingType)))
temp.table <- data.table(temp.table)
single.sbjs <- as.character(temp.table[CuriosityRatingType == "Low" & Freq < 2]$SubjectNo)

t.test(CorrContextAccuracy ~ CuriosityRatingType, data = Curiosity.Recall.Old.Order.CuriosityType[!SubjectNo %in% c(bad.ps.less.strict, single.sbjs) & ItemOrderType == "Early"], paired = T)
```

##### Simple effects in "Late"
```{r}
t.test(CorrContextAccuracy ~ CuriosityRatingType, data = Curiosity.Recall.Old.Order.CuriosityType[!SubjectNo %in% c(bad.ps.less.strict, single.sbjs) & ItemOrderType == "Late"], paired = T)
```

However, for the late items, the recollection was better for *low* curiosity significantly, *p* = 0.034.

#### Linear trend respectivley for High and Low curiosity groups
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6}
ggplot(Curiosity.Recall.Old.CuriosityType[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrContextAccuracy)) + theme_classic() +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, aes(group = CuriosityRatingType, colour = CuriosityRatingType), position = position_dodge(width = 0.4), size = 0.5, alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "point", aes(group = CuriosityRatingType, colour = CuriosityRatingType), position = position_dodge(width = 0.4), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = CuriosityRatingType, colour = CuriosityRatingType)) +
  labs(x = "Item order", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Low Curiosity", "High Curiosity")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

```{r}
Curiosity.Recall.Old.CuriosityType$ItemOrderCt <- Curiosity.Recall.Old.CuriosityType$ItemOrder - 1

baseline <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo/CuriosityRatingType, data = Curiosity.Recall.Old.CuriosityType[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

orderModel.linear.1   <- update(baseline,              .~. + ItemOrderCt)
orderModel.linear.2   <- update(orderModel.linear.1,   random = ~ 1 + ItemOrderCt|SubjectNo/CuriosityRatingType)

curiosityModel        <- update(orderModel.linear.2,   .~. + CuriosityRatingType)
curiosityModel.linear <- update(curiosityModel,        .~. + CuriosityRatingType : ItemOrderCt)

anova(baseline, orderModel.linear.1, orderModel.linear.2, curiosityModel, curiosityModel.linear)
```
Significant interaction, *p* < 0.01.

```{r}
summary(curiosityModel.linear)
```
No significant difference for the first item between high and low curiosity groups.

```{r}
highModel <- lme(CorrContextAccuracy ~ ItemOrderCt, random = ~ 1|SubjectNo/CuriosityRatingType, data = Curiosity.Recall.Old.CuriosityType[!SubjectNo %in% bad.ps.less.strict & CuriosityRatingType == "High"], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)
summary(highModel)$tTable
```
For high curiosity, performance decreased significantly over items, *p* < 0.01.

```{r}
lowModel <- lme(CorrContextAccuracy ~ ItemOrderCt, random = ~ 1|SubjectNo/CuriosityRatingType, data = Curiosity.Recall.Old.CuriosityType[!SubjectNo %in% bad.ps.less.strict & CuriosityRatingType == "Low"], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)
summary(lowModel)$tTable
```
For low curiosity, performance increased over items, but not significantly.


#### Try another way to calculate "High" and "Low" curiosity groups
> "High" - higher than mean; "Low" - lower than mean

##### Compare "High" and "Low" groups
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3}
ggplot(Curiosity.Recall.Old.Collapsed.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = RatingGroup, y = CorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  labs(x = "Curiosity level", y = "Corrected hit rate")
```

```{r, echo = FALSE}
t.test(CorrContextAccuracy ~ RatingGroup, data = Curiosity.Recall.Old.Collapsed.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], paired = T)
```

```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6}
ggplot(Curiosity.Recall.Old.Order.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = RatingGroup, y = CorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity level", y = "Mean corrected hit rate") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

##### Stats on it
```{r, echo = FALSE}
baseline      <- lme(CorrContextAccuracy ~ 1, random = ~1| SubjectNo/RatingGroup/ItemOrderType, data = Curiosity.Recall.Old.Order.CuriosityType.MeanSep [!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE))

curiosityModel   <- update(baseline,         .~. + RatingGroup)
orderModel       <- update(curiosityModel,   .~. + ItemOrderType)
interactionModel <- update(orderModel,       .~. + RatingGroup : ItemOrderType)

anova(baseline, curiosityModel, orderModel, interactionModel)
```
Again, significant interaction.

```{r}
summary(interactionModel)$tTable
```
For the early items, the recollection was only marginally better for high curiosity, *p* = 0.067.

##### Simple effects in "Early"
```{r}
t.test(CorrContextAccuracy ~ RatingGroup, data = Curiosity.Recall.Old.Order.CuriosityType.MeanSep[!SubjectNo %in% c(bad.ps.less.strict) & ItemOrderType == "Early"], paired = T)
```

##### Simple effects in "Late"
```{r}
t.test(ContextAccuracy ~ RatingGroup, data = Curiosity.Recall.Old.Order.CuriosityType.MeanSep[!SubjectNo %in% c(bad.ps.less.strict) & ItemOrderType == "Late"], paired = T)
```

#### Linear trend respectivley for High and Low curiosity groups
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6}
ggplot(Curiosity.Recall.Old.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrContextAccuracy)) + theme_classic() +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, aes(group = RatingGroup, colour = RatingGroup), position = position_dodge(width = 0.4), size = 0.5, alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "point", aes(group = RatingGroup, colour = RatingGroup), position = position_dodge(width = 0.4), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = RatingGroup, colour = RatingGroup)) +
  labs(x = "Item order", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Low Curiosity", "High Curiosity")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

```{r}
Curiosity.Recall.Old.CuriosityType.MeanSep$ItemOrderCt <- Curiosity.Recall.Old.CuriosityType.MeanSep$ItemOrder - 1

baseline <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo/RatingGroup, data = Curiosity.Recall.Old.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

orderModel.linear.1   <- update(baseline,              .~. + ItemOrderCt)
# orderModel.linear.2   <- update(orderModel.linear.1,   random = ~ 1 + ItemOrderCt|SubjectNo/RatingGroup)

curiosityModel        <- update(orderModel.linear.1,   .~. + RatingGroup)
curiosityModel.linear <- update(curiosityModel,        .~. + RatingGroup : ItemOrderCt)

anova(baseline, orderModel.linear.1, curiosityModel, curiosityModel.linear)
```

Significant interaction, *p* < 0.001.

```{r}
summary(curiosityModel.linear)$tTable
```
For the first item, the performance was significantly better for high curiosity, *p* = 0.012.

```{r}
highModel <- lme(CorrContextAccuracy ~ ItemOrderCt, random = ~ 1|SubjectNo/RatingGroup, data = Curiosity.Recall.Old.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict & RatingGroup == "High"], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)
summary(highModel)$tTable
```
For high curiosity, performance decreased significantly over items, *p* < 0.01.

```{r}
lowModel <- lme(CorrContextAccuracy ~ ItemOrderCt, random = ~ 1|SubjectNo/RatingGroup, data = Curiosity.Recall.Old.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict & RatingGroup == "Low"], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)
summary(lowModel)$tTable
```
For low curiosity, performance increased significantly over items, *p* = 0.033.


# Curiosity and novelty
## Testing the effects of curiosity rating and novelty together
Build models with context memory accuracy as $y$ and examine effects of curiosity rating and novelty together by including them into the model separately and then comparing the model fits. 

* `baseline.rating` is the model with only an intercept, showing no change in context memory as a function of curiosity rating. 
* `rating.intercept` is the model with $\small CuriosityRating$ included, showing the effects of curiosity rating. 
* `rating.slope` is the model allowing the slope (i.e., relationship between context memory and curiosity rating) varying across participants. 
* `novelty.model` is the model with $\small Group$ (i.e., *familiar* and *novel* rooms) included, examining the effects of novelty of the rooms on the intercept (i.e., mean context memory accuracy at the 1st order or overall if no difference in slope). 
* `interaction.model` is the model with interaction between $\small Group$ and $\small CuriosityRating$ included, examining the effects of novelty on how context memory accuracy changes as a function of item order. 

``` {r, echo = FALSE}
baseline.rating <- lme(CorrContextAccuracy ~ 1, random = ~ 1|SubjectNo/Group, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + CuriosityRating)
# rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating|SubjectNo/Group)

novelty.model.1 <- update(rating.intercept, .~. + Group)

interaction.model <- update(novelty.model.1, .~. + Group:CuriosityRating)

anova(baseline.rating, rating.intercept, novelty.model.1, interaction.model)
```

### Check the complete model
```{r, echo=FALSE}
summary(interaction.model )$tTable
```


## What about the other way around
```{r, echo=FALSE}
novelty.model.1 <- update(baseline.context, .~. + Group)

rating.model.1 <- update(novelty.model.1, .~. + CuriosityRating)
rating.model.2 <- update(rating.model.1, random = ~ 1 + CuriosityRating|SubjectNo/Group)

interaction.model <- update(rating.model.2, .~. + Group:CuriosityRating)

anova(baseline.context, novelty.model.1, rating.model.1, rating.model.2, interaction.model)
```

## Check the collapsed data
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6}
ggplot(Curiosity.Recall.Old.Interaction.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = RatingGroup, y = CorrContextAccuracy)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Mean corrected hit rate") +
  facet_wrap( ~ Group, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```
Sample sizes differed hugely between conditions, so hard to compare.

<!-- #### Stats on it -->
```{r, echo = FALSE, eval = FALSE, include = FALSE}
baseline <- lme(CorrContextAccuracy ~ 1, random = ~ 1| SubjectNo/RatingGroup/Group, data = Curiosity.Recall.Old.Interaction.MeanSep[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE))

curiosityModel   <- update(baseline, .~. + RatingGroup)
noveltyModel     <- update(curiosityModel, .~. + Group)
interactionModel <- update(noveltyModel,  .~. + RatingGroup : Group)

anova(baseline, curiosityModel, noveltyModel, interactionModel)

summary(interactionModel)$tTable
```

# Curiosity effects in the *Novel* group
## Mean accuracy of item memory as a function of curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict & Group == "Novel"], aes(x = CuriosityRating, y = CorrContextAccuracy)) +
  stat_summary(fun.data = mean_cl_boot, na.rm = T, geom = "errorbar", width = 0.2) +
  stat_summary(fun.y = mean, na.rm = T, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(1:7)) +
  labs(x = "Curiosity rating", y = "Mean corrected hit rate") 
```