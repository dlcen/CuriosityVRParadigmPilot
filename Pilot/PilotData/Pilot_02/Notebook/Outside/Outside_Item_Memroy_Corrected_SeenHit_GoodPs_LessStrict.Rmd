---
title: "Outside Item Recollection"
output:
  html_document:
    fig_caption: yes
    highlight: default
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 5
    toc_float: 
      collapsed: true
      smooth_scroll: true
  header-includes:
  - \usepackage{xcolor}
  - \usepackage{framed}
  - \usepackage{fontspec}
  html_notebook:
    
    fig_caption: yes
    highlight: default
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 5
    toc_float: true
  pdf_document:
    fig_caption: yes
    highlight: default
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 5
---

\setmainfont{Calibri}
\colorlet{shadecolor}{gray!10}
\color{red}

<style>
body {
  font-size: 15px;
}

p.caption {
  font-weight: 500;
  font-style: italic;
}

code.r {
  font-family: Consolas, Monaco, monospace;
  font-size: 12px;
}
</style>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(kableExtra)
options(knitr.table.format = "html", cache=TRUE) # "latex" for PDF
```

> This notebook focuses on recollection for items outside the rooms.
> The hit rate data was corrected by subtracting false alarm rate from the original hit rate. In those cases when the corrected hit rate was negative (the hit rate was lower than the false alarm rate), the corrected hit rate was forced to be 0. 
> Note that the corrected hit rate is meaningful only when it was calculated as a mean.


```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(data.table); library(ggplot2); library(nlme); #library(ez)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# load("~/Documents/OneDrive - Cardiff University/Projects/VRCuriosityMemory/PilotData/RawData/Pilot_02/.RData")
load("D:/OneDirve_CU/OneDrive - Cardiff University/Projects/VRCuriosityMemory/PilotData/RawData/Pilot_02/.RData")
# load("D:/Danlu.C - CU OneDrive/OneDrive - Cardiff University/VRCuriosityMemory/PilotData/RawData/Pilot_02/.RData")
```

# Overview of the data
The rooms were separated into *Familiar* and *Novel* groups. Participants explored the *Familiar* group at the beginning of the experiment. In the encoding task, they visisted both *Familiar* and *Novel* groups. Their item memory (i.e., in which room they saw the item) was tested in the memory test the next day of the encoding task. **Novelty benefits** on the item memory are examined by comparing the memory performance between the *Familiar* and *Novel* groups. 

Participants were also asked to rate how curious they felt about the room. The higher the rating is means they felt more curious about the room. Item memory as a function of curiosity rating was also examined for **curiosity benefits**.

Questionnaires on personal traits regarding curiosity were also administered to participants before the encoding task (i.e., PC and EC). Scores on these questionnaires might be able to predict the item memory or its relationship with curiosity rating. 

# Visual inspection of the data

```{r Find out bad participants, echo=FALSE}
bad.ps.less.strict <- NULL

for (this.p in participant.folders) {
  this.data <- Curiosity.Recall.Freq.Rsp[SubjectNo == this.p]
  if (this.data[CorrObjResp == "New" & Response == "Seen"]$Frequency > this.data[CorrObjResp == "Seen" & Response == "Seen"]$Frequency) {
    bad.ps.less.strict <- c(bad.ps.less.strict, this.p)
  }
}

Curiosity.Recall.Freq.Grp$Type2 <- "Good"
Curiosity.Recall.Freq.Grp[SubjectNo %in% bad.ps.less.strict]$Type2 <- "Bad"

Curiosity.Recall.Freq.Rsp$Type2 <- "Good"
Curiosity.Recall.Freq.Rsp[SubjectNo %in% bad.ps.less.strict]$Type2 <- "Bad"
```


```{r, echo=FALSE, fig.align='center', fig.width=7.2, fig.height=9, fig.cap='Figure 1. Response frequency for new and old items respectively for each individual participant.'}
ggplot(Curiosity.Recall.Freq.Rsp, aes(x = Response, y = Frequency, group = CorrObjResp)) + theme_bw() +
  geom_rect(data = subset(Curiosity.Recall.Freq.Rsp,Type2 == 'Bad'),aes(fill = Type2),xmin = -Inf,xmax = Inf,ymin = -Inf,ymax = Inf,alpha = 0.1) +
  geom_point(size = 3, aes(color = CorrObjResp)) +
  geom_line(size = 0.75, aes(color = CorrObjResp)) +
  scale_x_discrete("Response", limits = c("Seen", "Familiar", "New")) +
  scale_color_brewer(palette = "Set1", labels = c("New items", "Old items")) +
  facet_wrap( ~ SubjectNo, ncol = 4) + 
  theme(strip.text = element_text(face = 'bold'),
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 0.5) )
```


```{r, echo=FALSE, fig.align='center', fig.width=7.2, fig.height=9, fig.cap='Figure 2. Response frequency for new and old items respectively for each Familiar/Novel group and each individual participant.'}
ggplot(Curiosity.Recall.Freq.Grp, aes(x = Response, y = Frequency, group = Group)) + theme_bw() +
  geom_rect(data = subset(Curiosity.Recall.Freq.Grp,Type2 == 'Bad'),aes(fill = Type2),xmin = -Inf,xmax = Inf,ymin = -Inf,ymax = Inf,alpha = 0.1) +
  geom_point(size = 3, aes(color = Group)) +
  geom_line(size = 0.75, aes(color = Group)) +
  scale_x_discrete("Response", limits = c("Seen", "Familiar", "New")) +
  scale_color_brewer(palette = "Set1", labels = c("Distractor", "In familiar room", "In novel room")) +
  facet_wrap( ~ SubjectNo, ncol = 4) + 
  theme(strip.text = element_text(face = 'bold'),
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 0.5) )
```

# Novelty Benefits 
We hypothesised that the memory for the items outside the rooms should be better for the *novel* rooms than for the *familiar* rooms. 

## Compare mean accuracy between *novel* and *familiar* rooms
```{r meanbars, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure 3. Mean hit rate for recollect response respectively for Familiar and Novel rooms.'}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict, .(MeanCorrSeenHit = mean(CorrSeenHit)), by = c("SubjectNo", "Group")], aes(x = Group, y = MeanCorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  labs(x = "Room novelty", y = "Mean Corrected Hit Rate")
```

```{r, echo = FALSE}
t.test(MeanCorrSeenHit ~ Group, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict, .(MeanCorrSeenHit = mean(CorrSeenHit)), by = c("SubjectNo", "Group")], paired = TRUE)
```
The mean corrected hit rate for items was higher for the *Novel* rooms than for the *Familiar* rooms; however, the difference was short of significance, *t*(39) = -1.85, *p* = 0.072.

## Item memory as a function of item order
### Visualise the data
#### Nonlinear
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6, fig.cap="Figure 4. Mean hit rate for seen response at each order (i.e., 1st, 2nd, 3rd, etc), respectively for familiar and novel rooms."}
ggplot(Curiosity.Recall.Old.Novelty[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "point", aes(group = Group, colour = Group), position = position_dodge(width = 0.2), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, aes(group = Group, colour = Group)) +
  labs(x = "Item order", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

#### Linear
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6, fig.cap="Figure 5. Mean hit rate for seen response at each order (i.e., 1st, 2nd, 3rd, etc), respectively for familiar and novel rooms."}
ggplot(Curiosity.Recall.Old.Novelty[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrSeenHit)) + 
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, aes(group = Group, colour = Group), position = position_dodge(width = 0.25), size = 0.5, alpha = 0.25) +
  stat_summary(fun.y = mean, geom = "point", aes(group = Group, colour = Group), position = position_dodge(width = 0.25), size = 2, alpha = 0.75) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = Group, colour = Group)) +
  labs(x = "Item order", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Familiar rooms", "Novel rooms")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```
From the figures above, we can see that the difference between the *Familiar* and *Novel* rooms decreased as the participant got closer to the room. The change was smaller for the *Familiar* rooms than the *Novel* room. 

### Testing novelty effects by fitting mixed models
Build models with item memory accuracy as $y$ and then examine effects of novelty by comparing the models. 

* `baseline.item` is the model with only an intercept. 
* `order.model.linear.1` is the model with $\small ItemOrder$ included, showing the effects of item order. 
* `order.model.linear.2` is the model allowing the slope (i.e., relationship between item memory and item order) varying across participants. 
* `order.model.quad.1` and `order.model.quad.2` repectively included $\small ItemOrder^2$ and related random effects. 
* `novelty.model.1` is the model with $\small Group$ (i.e., *familiar* and *novel* rooms) included, examining the effects of novelty of the rooms on the intercept (i.e., mean item memory accuracy at the 1st order or overall if no difference in slope). 
* `novelty.model.2` is the model with interaction between $\small Group$ and $\small ItemOrder$ included, examining the effects of novelty on how item memory accuracy changes as a function of item order. 
* `novelty.model.3` included interaction between $\small Group$ and $\small ItemOrder^2$ for inestigating whether the pattern changes between *familiar* and *novel* groups. 

```{r, echo=FALSE}
Curiosity.Recall.Old.Novelty$ItemOrderCt <- Curiosity.Recall.Old.Novelty$ItemOrder - 1
# Set the Novel as the baseline
Curiosity.Recall.Old.Novelty$Group <- factor(Curiosity.Recall.Old.Novelty$Group)
contrasts(Curiosity.Recall.Old.Novelty$Group) <- c(1, 0)
```

```{r}
baseline.item <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo/Group, data = Curiosity.Recall.Old.Novelty[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

order.model.linear.1 <- update(baseline.item, .~. + ItemOrderCt)
# order.model.linear.2 <- update(order.model.linear.1, random = ~ 1 + ItemOrderCt|SubjectNo/Group)

# order.model.quad.1   <- update(order.model.linear.1, .~. + I(ItemOrderCt^2))
# order.model.quad.2   <- update(order.model.quad.1, random = ~ 1 + ItemOrderCt + I(ItemOrderCt^2)|SubjectNo/Group)

novelty.model.1 <- update(order.model.linear.1, .~. + Group)
novelty.model.2 <- update(novelty.model.1, .~. + Group:ItemOrderCt)
# novelty.model.3 <- update(novelty.model.2, .~. + Group:I(ItemOrderCt^2))

anova(baseline.item, order.model.linear.1, novelty.model.1, novelty.model.2)
```

There was a significant main effect for item order, indicating that recollection of the items encountered at the beginning of the pathway was better than those encountered later. 

As what we found on the mean hit rates above, the difference between the *Familiar* and *Novel* rooms was only marginally significant (*p* = 0.083).

However, we didn't have a significant interaction between room novelty and item order (*p* = 0.18), so the difference in the change rate of recollection did not seem to differ significantly between *Familiar* and *Novel* rooms.

#### Parameter estimation for the complete model
```{r, echo=FALSE}
summary(novelty.model.2, correlation = FALSE)
```
The parameters in the complete model tells us that for the *Novel* rooms, the recollection significantly decreased over the items encountered along the pathway *b* = -0.024, *t*(398) = -2.79, *p* < 0.01, and the recollection for the first item encountered on the pathway was significantly between for the *Novel* rooms than for the *Familiar* rooms, *b* = 0.079, *t*(39) = 2.11, *p* = 0.41.

### Collapse the order and novelty group

```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6, fig.cap='Figure 6. I collapsed the data into two order groups. The first three items encountered on the pathway were grouped into the Early group; and the last three items into the Late group.'}
ggplot(Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict], aes(x = Group, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Room Novelty", y = "Corrected Hit Rate") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

#### Stats on it
I used `lme` to calculate a repeated-measures ANOVA, as suggested by Andy Field's *Discovering Statistics using R*.

```{r, echo = FALSE}
# Make the Novel as the baseline
Curiosity.Recall.Old.Order.Novelty$Group <- factor(Curiosity.Recall.Old.Order.Novelty$Group)
contrasts( Curiosity.Recall.Old.Order.Novelty$Group) <- c(1, 0)
```

```{r, echo = FALSE}
baseline      <- lme(CorrSeenHit ~ 1, random = ~1|SubjectNo/Group/ItemOrderType, data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict], method = "ML")
NoveltyModel  <- update(baseline,     .~. + Group)
OrderModel    <- update(NoveltyModel,     .~. + ItemOrderType)
repeatedModel <- update(OrderModel, .~. + ItemOrderType : Group)

anova(baseline, NoveltyModel, OrderModel, repeatedModel)
```
Comparing the models revealed a marginally significant main effect of *Novelty*, ${\chi}^2$(1) = 3.21, *p* = 0.073.

##### Parameters in the complete model
``` {r, echo = FALSE}
summary(repeatedModel)
```
Parameteres in the complete model tells us that, for the *Early* group, the recollection was significantly better than the *Late* group, *t*(39) = 2.33, *p* = 0.025. For *Novel* rooms, recollection of the items in the *Early* group was significantly better for the items in the *Late* group, *t*(78) = -2.10, *p* = 0.039. 

<!-- ##### Using `ezANOVA` -->
```{r, eval = FALSE, include = FALSE}
repeatedModel<-ezANOVA(data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict], dv = .(CorrSeenHit), wid = .(SubjectNo),  within = .(Group, ItemOrderType), type = 3, detailed = TRUE)
options(digits = 3)
repeatedModel
```

##### Simple effects for the "early" group
```{r, echo = FALSE}
t.test(CorrSeenHit ~ Group, data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict & ItemOrderType == "Early"], paired = TRUE)
```
In consistent with the `lme` modelling above, comparing the mean corrected hit rate of the *Early* groups between the *Familiar* and *Novel* rooms revealed a significant difference, *t*(39) = -2.48, *p* = 0.018.

##### Simple effects for the "late" group
```{r, echo = FALSE}
t.test(CorrSeenHit ~ Group, data = Curiosity.Recall.Old.Order.Novelty[!SubjectNo %in% bad.ps.less.strict & ItemOrderType == "Late"], paired = TRUE)
```
No difference for the *Late* groups. 

# Curiosity Benefits
We hypothesed that the higher the curiosity rating was, the better the item memory should be.

## Mean accuracy of item memory as a function of curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3, fig.cap='Figure 7. Mean hit rate as a function of curiosity rating.'}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRating, y = CorrSeenHit)) +
  stat_summary(fun.data = mean_cl_boot, na.rm = T, geom = "errorbar", width = 0.2) +
  stat_summary(fun.y = mean, na.rm = T, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(1:7)) +
  labs(x = "Curiosity rating", y = "Mean corrected hit rate") 
```

### Testing the relationship between mean accuracy of item memory and curiosity rating
Build models with item memory accuracy as $y$ and examine effects of curiosity rating by including it into the model and then comparing the model fits. 

* `baseline.rating` is the model with only an intercept, showing no change in item memory as a function of curiosity rating. 
* `rating.intercept` is the model with $\small CuriosityRating$ included, showing the effects of curiosity rating. 
* `rating.slope` is the model allowing the slope (i.e., relationship between item memory and curiosity rating) varying across participants. 

```{r, echo = FALSE}
baseline.rating <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + CuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating|SubjectNo)

anova(baseline.rating, rating.intercept, rating.slope)
```
There was a significant fixed effect of curiosity rating, ${\chi}^2(1)$ = 6.11, *p* = 0.013. 

Allowing relationship between curiosity rating and recollection performance to vary across participants (*random effect*) improved the model fit, but not significantly, ${\chi}^2$(2) = 4.51, *p* = 0.10.

#### Parameter estimates in the complete model
```{r}
summary(rating.slope)
```
The slope was significantly larger than 0, *b* = 0.016, *t*(195) = 2.08, *p* = 0.039, indicating a positive relationship between curiosity rating and recollection performance (see the figure below).

#### Plot the model
```{r, echo = FALSE, fig.align='center', fig.width=6.4, fig.height=4.8}
fixef.curiosity.item <- fixef(rating.slope)
fit.curiosity.item <- fixef.curiosity.item[[1]] + c(1:7) * fixef.curiosity.item[[2]]
plot(c(1:7), fit.curiosity.item, ylim = c(0, 1), type = "b", ylab = "predicted hit rate", xlab = "curiosity rating")
title("Relationship between Curiosity Rating and Hit Rate")
```

## Does the relationship with curiosity rating depend on item order?
### Curiosity Rating
```{r, echo=FALSE, fig.align='center', fig.width=10.8, fig.height=3, fig.cap='Figure 8. Mean hit rate across participants as a function of curiosity rating for each order at which the item was seen (I.e., 1st, 2nd, 3rd, etc.).'}
ggplot(Curiosity.Recall.Old.CuriosityRating[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRating, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  labs(x = "Curiosity rating", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:7)) +
  facet_wrap( ~ ItemOrder, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

### Normalised Curiosity Rating
```{r, echo=FALSE, fig.align='center', fig.width=10.8, fig.height=3, fig.cap='Figure 9. Mean hit rate across participants as a function of normalised curiosity rating for each order at which the item was seen (I.e., 1st, 2nd, 3rd, etc.).'}
ggplot(Curiosity.Recall.Old.CuriosityRatingN[!SubjectNo %in% bad.ps.less.strict], aes(x = NCuriosityRating, y = CorrSeenHit)) + theme_minimal() +
  # stat_summary(fun.y = mean, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE, aes(group = SubjectNo, colour = SubjectNo)) +
  labs(x = "Curiosity rating", y = "Corrected Hit Rate") +
  # scale_x_continuous(breaks = c(1:7)) +
  facet_wrap( ~ ItemOrder, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

### Testing the effect of item order
Build models with context memory accuracy as $y$ and examine effects of curiosity rating and item order by comparing the models. 

* `item.model.linear.1` is the model with $\small ItemOrder$ included (in addition to $\small CuriosityRating$), showing the effects of item order. 
* `item.model.linear.2` is the model allowing the slope (i.e., relationship between item memory and item order) varying across participants. 
* `item.model.linear.3` is the model with the interaction between $\small ItemOrders$ and $\small CuriosityRating$ included.
* `item.model.quad.1` and `item.model.quad.2` repectively included $\small ItemOrder^2$ and related random effects. 
* `item.model.quad.3` included interaction between $\small ItemOrder^2$ and $\small CuriosityRating$ for inestigating whether the pattern changes as a function of *curiosity rating*. 

#### Original curiosity ratings
```{r}
baseline.item <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo, data = Curiosity.Recall.Old.CuriosityRating[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.item, .~. + CuriosityRating)
# rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating | SubjectNo)

item.model.linear.1     <- update(rating.intercept, .~. + ItemOrder)
# item.model.linear.2     <- update(item.model.linear.1, random = ~1+CuriosityRating + ItemOrder|SubjectNo)
item.model.linear.3     <- update(item.model.linear.1, .~. + ItemOrder : CuriosityRating)

# item.model.quad.1       <- update(item.model.linear.3, .~. + I(ItemOrder^2))
# item.model.quad.2       <- update(item.model.quad.1,   random = ~1+CuriosityRating+ItemOrder+I(ItemOrder^2)|SubjectNo) # excluding this model because singularity issue (perhaps the model was getting too complex to be calculated)
# item.model.quad.3       <- update(item.model.quad.2,   .~. + I(ItemOrder^2) : CuriosityRating)

anova(baseline.item, rating.intercept, item.model.linear.1, item.model.linear.3) #, item.model.quad.1, item.model.quad.2, item.model.quad.3)
```
There was a marginally significant main effect of curiosity rating, ${\chi}^2$(1) = 3.45, *p* = 0.063, and significant main effect of item order, ${\chi}^2(1)$ = 4.87, *p* = 0.027. No significantly interaction was found between curiosity rating and item order, *p* = 0.95.

##### Parameter estimation for the optimal model
```{r}
summary(item.model.linear.1 )
```

#### **Normalised** curiosity ratings
Here **normalised** curiosity ratings was used, just to see if there would be any difference.
```{r}
baseline.item <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo, data = Curiosity.Recall.Old.CuriosityRatingN[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.item, .~. + NCuriosityRating)
# rating.slope     <- update(rating.intercept, random = ~ 1 + NCuriosityRating | SubjectNo)

item.model.linear.1     <- update(rating.intercept, .~. + ItemOrder)
# item.model.linear.2     <- update(item.model.linear.1, random = ~1+NCuriosityRating + ItemOrder|SubjectNo)
item.model.linear.3     <- update(item.model.linear.1, .~. + ItemOrder : NCuriosityRating)

# item.model.quad.1       <- update(item.model.linear.3, .~. + I(ItemOrder^2))
# item.model.quad.2       <- update(item.model.quad.1,   random = ~1+CuriosityRating+ItemOrder+I(ItemOrder^2)|SubjectNo) # excluding this model because singularity issue (perhaps the model was getting too complex to be calculated)
# item.model.quad.3       <- update(item.model.quad.2,   .~. + I(ItemOrder^2) : CuriosityRating)

anova(baseline.item, rating.intercept,item.model.linear.1, item.model.linear.3) #, item.model.quad.1, item.model.quad.2, item.model.quad.3)
```

##### Parameter estimation for the optimal model
```{r}
summary(item.model.linear.1 )
```
Results are similar to the models on the original curiosity ratings.

### Collapse the order and curiosity rating
#### Collapsing data into *Low* curiosity (ratings of 1 - 3) and *High* curiosity (ratings of 4 - 7) groups.
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6, fig.cap='Figure 9. Mean hit rate as a function of item order (Early vs. Late) and curiosity (Low vs. High).'}
ggplot(Curiosity.Recall.Old.Order.CuriosityType[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRatingType, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Hit rate for recollection") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

##### Stats on it
Using `lme`
```{r, echo = FALSE}
baseline         <- lme(CorrSeenHit ~ 1, random = ~ 1| SubjectNo/CuriosityRatingType/ItemOrderType, data = Curiosity.Recall.Old.Order.CuriosityType[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE))

curiosityModel   <- update(baseline,         .~. + CuriosityRatingType)
orderModel       <- update(curiosityModel,   .~. + ItemOrderType)
interactionModel <- update(orderModel,       .~. + CuriosityRatingType : ItemOrderType )

anova(baseline, curiosityModel, orderModel, interactionModel)
```
There was a significant main effect of curiosity rating, ${\chi}^2$(1) = 5.08, *p* = 0.024.

No significant interaction.

```{r, eval = FALSE}
summary(interactionModel)
```

##### Linear trend respectivley for High and Low curiosity groups
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6}
ggplot(Curiosity.Recall.Old.CuriosityType[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrSeenHit)) + theme_classic() +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, aes(group = CuriosityRatingType, colour = CuriosityRatingType), position = position_dodge(width = 0.4), size = 0.5, alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "point", aes(group = CuriosityRatingType, colour = CuriosityRatingType), position = position_dodge(width = 0.4), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = CuriosityRatingType, colour = CuriosityRatingType)) +
  labs(x = "Item order", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Low Curiosity", "High Curiosity")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

```{r}
baseline           <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo/CuriosityRatingType, data = Curiosity.Recall.Old.CuriosityType[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim"))

orderModel.int     <- update(baseline,         .~. + ItemOrder)
orderModel.slp     <- update(orderModel.int,   random = ~ 1 + ItemOrder|SubjectNo/CuriosityRatingType )

curiosityModel     <- update(orderModel.slp,   .~. + CuriosityRatingType)
interactionModel   <- update(curiosityModel,   .~. + CuriosityRatingType : ItemOrder)

anova(baseline, orderModel.int, orderModel.slp, curiosityModel, interactionModel)
```

There was a main effect of curiosity, *p* = 0.041.

#### Try another way to calculate "High" and "Low" curiosity groups
> "High" - higher than mean; "Low" - lower than mean

##### First compare the difference between the two curiosity groups.
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3}
ggplot(Curiosity.Recall.Old.Collapsed.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = RatingGroup, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  labs(x = "Curiosity level", y = "Mean Corrected Hit Rate")
```

```{r, echo = FALSE}
t.test(CorrSeenHit ~ RatingGroup, data = Curiosity.Recall.Old.Collapsed.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], paired = T)
```
Recollection was better for the *High curiosity* group than the *Low curiosity* group, *t*(39) = 2.04, *p* = 0.048.

##### Relationship with item order
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6}
ggplot(Curiosity.Recall.Old.Order.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = RatingGroup, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Corrected hit rate for recollection") +
  facet_wrap( ~ ItemOrderType, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

##### Stats on it
```{r, echo = FALSE}
baseline          <- lme(CorrSeenHit ~ 1, random = ~ 1| SubjectNo/RatingGroup/ItemOrderType, data = Curiosity.Recall.Old.Order.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action=na.exclude)

ratingModel       <- update(baseline,     .~. + RatingGroup)
orderModel        <- update(ratingModel,  .~. + ItemOrderType)

interactionModel  <- update(orderModel,   .~. + RatingGroup : ItemOrderType)

anova(baseline, ratingModel, orderModel, interactionModel)
```
There was a marginally significant main effect of curiosity rating, ${\chi}^2$(1) = 3.71, *p* = 0.054.

No interaction.

#### Linear trend respectivley for High and Low curiosity groups
```{r, echo = FALSE, fig.align= 'center', fig.width=4.8, fig.height=3.6}
ggplot(Curiosity.Recall.Old.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = ItemOrder, y = CorrSeenHit)) + theme_classic() +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, aes(group = RatingGroup, colour = RatingGroup), position = position_dodge(width = 0.4), size = 0.5, alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "point", aes(group = RatingGroup, colour = RatingGroup), position = position_dodge(width = 0.4), size = 2) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = RatingGroup, colour = RatingGroup)) +
  labs(x = "Item order", y = "Corrected Hit Rate") +
  scale_x_continuous(breaks = c(1:6)) +
  scale_color_brewer(palette = "Set1", labels = c("Low Curiosity", "High Curiosity")) +
  theme(legend.title = element_blank(),
        axis.title = element_text(face = "bold"))
```

```{r}
Curiosity.Recall.Old.CuriosityType.MeanSep$ItemOrderCt <- Curiosity.Recall.Old.CuriosityType.MeanSep$ItemOrder - 1

baseline <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo/RatingGroup, data = Curiosity.Recall.Old.CuriosityType.MeanSep[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

orderModel.linear.1   <- update(baseline,              .~. + ItemOrderCt)
orderModel.linear.2   <- update(orderModel.linear.1,   random = ~ 1 + ItemOrderCt|SubjectNo/RatingGroup)

curiosityModel        <- update(orderModel.linear.2,   .~. + RatingGroup)
curiosityModel.linear <- update(curiosityModel,        .~. + RatingGroup : ItemOrderCt)

anova(baseline, orderModel.linear.1, orderModel.linear.2, curiosityModel, curiosityModel.linear)
```

Main effects of item order and curiosity, but no interaction.

# Curiosity and novelty

`r text_spec("TODO; will check better way of doing the analysis.", background = "#D05A6E", color = "white", bold = T, font_size = 15)`.

At the moment, I just ran the model building in two opposite orders, and to see how the degree of model fit improvement changed between two orders.

## Testing the effects of curiosity rating and novelty together
Build models with item memory accuracy as $y$ and examine effects of curiosity rating and novelty together by including them into the model separately and then comparing the model fits. 

* `baseline.rating` is the model with only an intercept, showing no change in item memory as a function of curiosity rating. 
* `rating.intercept` is the model with $\small CuriosityRating$ included, showing the effects of curiosity rating. 
* `rating.slope` is the model allowing the slope (i.e., relationship between item memory and curiosity rating) varying across participants. 
* `novelty.model` is the model with $\small Group$ (i.e., *familiar* and *novel* rooms) included, examining the effects of novelty of the rooms on the intercept (i.e., mean item memory accuracy at the 1st order or overall if no difference in slope). 
* `interaction.model` is the model with interaction between $\small Group$ and $\small CuriosityRating$ included, examining the effects of novelty on how item memory accuracy changes as a function of item order. 

``` {r, echo = FALSE}
baseline.rating <- lme(CorrSeenHit ~ 1, random = ~ 1|SubjectNo/Group, data = OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict], method = "ML", control = list(msMaxIter = 500, opt = "optim", msVerbose=FALSE), na.action = na.exclude)

rating.intercept <- update(baseline.rating, .~. + CuriosityRating)
rating.slope     <- update(rating.intercept, random = ~ 1 + CuriosityRating|SubjectNo/Group)

novelty.model.1 <- update(rating.slope, .~. + Group)

interaction.model <- update(novelty.model.1, .~. + Group:CuriosityRating)

anova(baseline.rating, rating.intercept, rating.slope, novelty.model.1, interaction.model)
```
Again, there was a significant main effect of curiosity rating, ${\chi}^2$(1) = 5.79, *p* = 0.016. No interaction.


## What about the other way around
```{r, echo=FALSE}
novelty.model.1 <- update(baseline.rating, .~. + Group)

rating.model.1 <- update(novelty.model.1, .~. + CuriosityRating)
rating.model.2 <- update(rating.model.1, random = ~ 1 + CuriosityRating|SubjectNo/Group)

interaction.model <- update(rating.model.2, .~. + Group:CuriosityRating)

anova(baseline.rating, novelty.model.1, rating.model.1, rating.model.2, interaction.model)
```


## Check the collapsed data
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6}
ggplot(Curiosity.Recall.Old.Interaction[!SubjectNo %in% bad.ps.less.strict], aes(x = CuriosityRatingType, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Hit rate for recollection") +
  facet_wrap( ~ Group, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

### Alternative way to collapse data
```{r, echo=FALSE, fig.align='center', fig.width=4.2, fig.height=3.6}
ggplot(Curiosity.Recall.Old.Interaction.MeanSep[!SubjectNo %in% bad.ps.less.strict], aes(x = RatingGroup, y = CorrSeenHit)) +
  stat_summary(fun.y = mean, geom = "bar", size = 2, fill = "grey50") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, position = position_dodge(width = 0.9)) +
  labs(x = "Curiosity rating", y = "Corrected hit rate for recollection") +
  facet_wrap( ~ Group, nrow = 1) +
  theme(strip.text = element_text(face = "bold", size = 11),
        axis.title = element_text(face = "bold"))
```

#### Stats
Because there appeared to be different variance between groups, I used robust repeated measures ANOVA in `WRS2` package on the 20% trimmed means. However, this might not be a good approach as sample size might be quite small in some conditions. `r text_spec("TODO; will check better way of doing the analysis.", background = "#D05A6E", color = "white", bold = T, font_size = 15)`. 

```{r}
library(WRS2)

bwtrim(CorrSeenHit ~ RatingGroup * Group, id = SubjectNo, data = Curiosity.Recall.Old.Interaction.MeanSep[!SubjectNo %in% bad.ps.less.strict])
```

# Curiosity effects in the *Novel* group
## Mean accuracy of item memory as a function of curiosity rating
```{r, echo=FALSE, fig.align='center', fig.width=3.6, fig.height=3}
ggplot(OutsideObjectsMemory[!SubjectNo %in% bad.ps.less.strict & Group == "Novel"], aes(x = CuriosityRating, y = CorrSeenHit)) +
  stat_summary(fun.data = mean_cl_boot, na.rm = T, geom = "errorbar", width = 0.2) +
  stat_summary(fun.y = mean, na.rm = T, geom = "point", size = 2) +
  stat_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(1:7)) +
  labs(x = "Curiosity rating", y = "Mean hit rate for recollection") 
```